# ë„¤ì´ë²„ ë¸”ë¡œê·¸ ì»¨í…ì¸  ì¶”ì¶œ ì‹œìŠ¤í…œ êµ¬í˜„ ì›Œí¬í”Œë¡œìš°

## ğŸ¯ í”„ë¡œì íŠ¸ ê°œìš”
ë„¤ì´ë²„ ë¸”ë¡œê·¸ URLì„ ì…ë ¥ë°›ì•„ í•´ë‹¹ ë¸”ë¡œê·¸ì˜ ê¸€ê³¼ ì´ë¯¸ì§€ë¥¼ ìë™ìœ¼ë¡œ ì¶”ì¶œí•˜ê³  ì €ì¥í•˜ëŠ” ì‹œìŠ¤í…œ êµ¬í˜„

## ğŸ“‹ ìš”êµ¬ì‚¬í•­ ë¶„ì„
- **ì…ë ¥**: ë„¤ì´ë²„ ë¸”ë¡œê·¸ URL (blog.naver.com ë˜ëŠ” m.blog.naver.com)
- **ì¶œë ¥**: ë¸”ë¡œê·¸ ì œëª©, ë³¸ë¬¸ ë‚´ìš©, ì´ë¯¸ì§€ íŒŒì¼, ë©”íƒ€ë°ì´í„°
- **ì €ì¥ í˜•ì‹**: JSON, CSV, ë˜ëŠ” ë°ì´í„°ë² ì´ìŠ¤
- **ì„±ëŠ¥ ìš”êµ¬ì‚¬í•­**: Rate limiting, ì—ëŸ¬ ì²˜ë¦¬, ì¬ì‹œë„ ë¡œì§

## ğŸ—ï¸ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

### ê¸°ìˆ  ìŠ¤íƒ ì„ íƒ
```
Primary Stack:
- Language: Python 3.8+
- Web Scraping: Selenium WebDriver ë˜ëŠ” Playwright
- HTML Parsing: BeautifulSoup4
- HTTP Requests: requests, aiohttp (ë¹„ë™ê¸°)
- Data Storage: SQLite, JSON
- Image Processing: Pillow
- Testing: pytest, unittest
```

### Alternative Stack (JavaScript):
```
- Language: Node.js
- Web Scraping: Puppeteer, Playwright
- HTML Parsing: Cheerio
- HTTP Requests: axios
- Data Storage: MongoDB, JSON
- Testing: Jest, Mocha
```

## ğŸ“… êµ¬í˜„ ë‹¨ê³„ë³„ ì›Œí¬í”Œë¡œìš°

### Phase 1: í”„ë¡œì íŠ¸ ì´ˆê¸° ì„¤ì • (Day 1)
#### 1.1 í”„ë¡œì íŠ¸ êµ¬ì¡° ìƒì„±
```
naver_blog_scraper/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ scraper/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ blog_scraper.py
â”‚   â”‚   â”œâ”€â”€ url_parser.py
â”‚   â”‚   â””â”€â”€ content_extractor.py
â”‚   â”œâ”€â”€ storage/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ file_storage.py
â”‚   â”‚   â””â”€â”€ database_storage.py
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ rate_limiter.py
â”‚   â”‚   â””â”€â”€ error_handler.py
â”‚   â””â”€â”€ config/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ settings.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_scraper.py
â”‚   â””â”€â”€ test_storage.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ images/
â”‚   â””â”€â”€ exports/
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ README.md
â””â”€â”€ .env.example
```

#### 1.2 í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
```python
# requirements.txt
selenium==4.15.0
beautifulsoup4==4.12.2
requests==2.31.0
Pillow==10.1.0
python-dotenv==1.0.0
lxml==4.9.3
aiohttp==3.9.0
playwright==1.40.0
pandas==2.1.3
sqlite3
pytest==7.4.3
```

### Phase 2: URL íŒŒì‹± ë° ê²€ì¦ ëª¨ë“ˆ (Day 2)
#### 2.1 URL Parser êµ¬í˜„
```python
# src/scraper/url_parser.py
import re
from urllib.parse import urlparse, parse_qs
from typing import Dict, Optional

class NaverBlogURLParser:
    """ë„¤ì´ë²„ ë¸”ë¡œê·¸ URL íŒŒì‹± ë° ê²€ì¦"""
    
    VALID_DOMAINS = ['blog.naver.com', 'm.blog.naver.com']
    URL_PATTERNS = {
        'pc': r'https?://blog\.naver\.com/([^/]+)/(\d+)',
        'mobile': r'https?://m\.blog\.naver\.com/([^/]+)/(\d+)',
        'post_view': r'https?://blog\.naver\.com/PostView\.naver\?blogId=([^&]+)&logNo=(\d+)'
    }
    
    def __init__(self):
        self.blog_id = None
        self.log_no = None
        
    def parse_url(self, url: str) -> Dict[str, str]:
        """URLì„ íŒŒì‹±í•˜ì—¬ blog_idì™€ log_no ì¶”ì¶œ"""
        parsed = urlparse(url)
        
        if parsed.netloc not in self.VALID_DOMAINS:
            raise ValueError(f"Invalid domain: {parsed.netloc}")
            
        # URL íŒ¨í„´ ë§¤ì¹­
        for pattern_name, pattern in self.URL_PATTERNS.items():
            match = re.match(pattern, url)
            if match:
                self.blog_id = match.group(1)
                self.log_no = match.group(2)
                break
                
        if not self.blog_id or not self.log_no:
            # Query parameter ë°©ì‹ ì²˜ë¦¬
            if 'PostView' in parsed.path:
                params = parse_qs(parsed.query)
                self.blog_id = params.get('blogId', [None])[0]
                self.log_no = params.get('logNo', [None])[0]
                
        if not self.blog_id or not self.log_no:
            raise ValueError("Unable to parse blog URL")
            
        return {
            'blog_id': self.blog_id,
            'log_no': self.log_no,
            'normalized_url': f"https://blog.naver.com/{self.blog_id}/{self.log_no}"
        }
```

### Phase 3: ì»¨í…ì¸  ì¶”ì¶œ ì—”ì§„ êµ¬í˜„ (Day 3-4)
#### 3.1 Selenium ê¸°ë°˜ ë™ì  ì»¨í…ì¸  ì¶”ì¶œ
```python
# src/scraper/content_extractor.py
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import time
from typing import Dict, List

class NaverBlogContentExtractor:
    """ë„¤ì´ë²„ ë¸”ë¡œê·¸ ì»¨í…ì¸  ì¶”ì¶œê¸°"""
    
    def __init__(self, headless: bool = True):
        self.driver = self._setup_driver(headless)
        
    def _setup_driver(self, headless: bool) -> webdriver.Chrome:
        """Selenium WebDriver ì„¤ì •"""
        options = webdriver.ChromeOptions()
        if headless:
            options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')
        options.add_argument('--window-size=1920,1080')
        options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
        
        return webdriver.Chrome(options=options)
        
    def extract_content(self, url: str) -> Dict:
        """ë¸”ë¡œê·¸ ì»¨í…ì¸  ì¶”ì¶œ"""
        self.driver.get(url)
        
        # iframe ì²˜ë¦¬ (ë„¤ì´ë²„ ë¸”ë¡œê·¸ëŠ” iframe ì‚¬ìš©)
        try:
            WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.ID, "mainFrame"))
            )
            self.driver.switch_to.frame("mainFrame")
        except:
            pass  # ì¼ë¶€ ë¸”ë¡œê·¸ëŠ” iframeì„ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
            
        # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
        time.sleep(2)
        
        # BeautifulSoupìœ¼ë¡œ íŒŒì‹±
        soup = BeautifulSoup(self.driver.page_source, 'html.parser')
        
        content = {
            'title': self._extract_title(soup),
            'author': self._extract_author(soup),
            'date': self._extract_date(soup),
            'content': self._extract_body(soup),
            'images': self._extract_images(soup),
            'tags': self._extract_tags(soup)
        }
        
        return content
        
    def _extract_title(self, soup: BeautifulSoup) -> str:
        """ì œëª© ì¶”ì¶œ"""
        selectors = [
            '.se-title-text',  # ìŠ¤ë§ˆíŠ¸ì—ë””í„° 3.0
            '.pcol1',          # êµ¬ë²„ì „
            'div.se_title',    # ìŠ¤ë§ˆíŠ¸ì—ë””í„° 2.0
            'h3.se-title'      # ëŒ€ì²´ ì„ íƒì
        ]
        
        for selector in selectors:
            title = soup.select_one(selector)
            if title:
                return title.get_text(strip=True)
        return "ì œëª© ì—†ìŒ"
        
    def _extract_body(self, soup: BeautifulSoup) -> str:
        """ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ"""
        selectors = [
            'div.se-main-container',  # ìŠ¤ë§ˆíŠ¸ì—ë””í„° 3.0
            'div#postViewArea',       # êµ¬ë²„ì „
            'div.se_component_wrap'   # ìŠ¤ë§ˆíŠ¸ì—ë””í„° 2.0
        ]
        
        for selector in selectors:
            content = soup.select_one(selector)
            if content:
                # í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
                return content.get_text(separator='\n', strip=True)
        return ""
        
    def _extract_images(self, soup: BeautifulSoup) -> List[Dict]:
        """ì´ë¯¸ì§€ URL ì¶”ì¶œ"""
        images = []
        img_tags = soup.find_all('img')
        
        for img in img_tags:
            src = img.get('src') or img.get('data-src')
            if src and not src.startswith('data:'):
                images.append({
                    'url': src,
                    'alt': img.get('alt', ''),
                    'title': img.get('title', '')
                })
                
        return images
```

#### 3.2 Playwright ëŒ€ì•ˆ êµ¬í˜„ (ë” ë¹ ë¥¸ ì„±ëŠ¥)
```python
# src/scraper/playwright_extractor.py
import asyncio
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
from typing import Dict, List

class PlaywrightBlogExtractor:
    """Playwrightë¥¼ ì‚¬ìš©í•œ ë¹„ë™ê¸° ì»¨í…ì¸  ì¶”ì¶œ"""
    
    async def extract_content(self, url: str) -> Dict:
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context(
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            )
            page = await context.new_page()
            
            await page.goto(url, wait_until='networkidle')
            
            # iframe ì²˜ë¦¬
            try:
                frame = page.frame('mainFrame')
                if frame:
                    content = await frame.content()
                else:
                    content = await page.content()
            except:
                content = await page.content()
                
            soup = BeautifulSoup(content, 'html.parser')
            
            result = {
                'title': self._extract_title(soup),
                'content': self._extract_body(soup),
                'images': self._extract_images(soup)
            }
            
            await browser.close()
            return result
```

### Phase 4: ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° ì €ì¥ (Day 5)
#### 4.1 ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë” êµ¬í˜„
```python
# src/scraper/image_downloader.py
import os
import requests
from PIL import Image
from io import BytesIO
from urllib.parse import urlparse, unquote
from typing import List, Dict
import hashlib

class ImageDownloader:
    """ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° ì €ì¥"""
    
    def __init__(self, save_dir: str = "data/images"):
        self.save_dir = save_dir
        os.makedirs(save_dir, exist_ok=True)
        
    def download_images(self, images: List[Dict], blog_id: str, log_no: str) -> List[Dict]:
        """ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° ë¡œì»¬ ì €ì¥"""
        saved_images = []
        blog_dir = os.path.join(self.save_dir, f"{blog_id}_{log_no}")
        os.makedirs(blog_dir, exist_ok=True)
        
        for idx, img_info in enumerate(images):
            try:
                response = requests.get(img_info['url'], timeout=10)
                if response.status_code == 200:
                    # ì´ë¯¸ì§€ íŒŒì¼ëª… ìƒì„±
                    ext = self._get_extension(img_info['url'])
                    filename = f"image_{idx:03d}{ext}"
                    filepath = os.path.join(blog_dir, filename)
                    
                    # ì´ë¯¸ì§€ ì €ì¥ ë° ìµœì í™”
                    img = Image.open(BytesIO(response.content))
                    
                    # ì´ë¯¸ì§€ í¬ê¸° ìµœì í™” (ì„ íƒì‚¬í•­)
                    if img.width > 1920:
                        img.thumbnail((1920, 1920), Image.Resampling.LANCZOS)
                        
                    img.save(filepath, optimize=True, quality=85)
                    
                    saved_images.append({
                        'original_url': img_info['url'],
                        'local_path': filepath,
                        'alt': img_info.get('alt', ''),
                        'size': os.path.getsize(filepath)
                    })
                    
            except Exception as e:
                print(f"Failed to download image: {e}")
                
        return saved_images
        
    def _get_extension(self, url: str) -> str:
        """URLì—ì„œ íŒŒì¼ í™•ì¥ì ì¶”ì¶œ"""
        path = urlparse(url).path
        ext = os.path.splitext(path)[1]
        return ext if ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp'] else '.jpg'
```

### Phase 5: ë°ì´í„° ì €ì¥ ë° Export (Day 6)
#### 5.1 ë‹¤ì–‘í•œ í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ì €ì¥
```python
# src/storage/data_storage.py
import json
import csv
import sqlite3
from datetime import datetime
from typing import Dict, List
import pandas as pd

class DataStorage:
    """ì¶”ì¶œëœ ë°ì´í„° ì €ì¥ ê´€ë¦¬"""
    
    def __init__(self, db_path: str = "data/blog_data.db"):
        self.db_path = db_path
        self._init_database()
        
    def _init_database(self):
        """SQLite ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS blog_posts (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                blog_id TEXT,
                log_no TEXT,
                title TEXT,
                author TEXT,
                date TEXT,
                content TEXT,
                images TEXT,
                tags TEXT,
                scraped_at TIMESTAMP,
                UNIQUE(blog_id, log_no)
            )
        ''')
        
        conn.commit()
        conn.close()
        
    def save_to_json(self, data: Dict, filepath: str):
        """JSON í˜•ì‹ìœ¼ë¡œ ì €ì¥"""
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
            
    def save_to_csv(self, data: List[Dict], filepath: str):
        """CSV í˜•ì‹ìœ¼ë¡œ ì €ì¥"""
        df = pd.DataFrame(data)
        df.to_csv(filepath, index=False, encoding='utf-8-sig')
        
    def save_to_database(self, data: Dict):
        """ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT OR REPLACE INTO blog_posts 
            (blog_id, log_no, title, author, date, content, images, tags, scraped_at)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            data.get('blog_id'),
            data.get('log_no'),
            data.get('title'),
            data.get('author'),
            data.get('date'),
            data.get('content'),
            json.dumps(data.get('images', [])),
            json.dumps(data.get('tags', [])),
            datetime.now()
        ))
        
        conn.commit()
        conn.close()
        
    def export_to_markdown(self, data: Dict, filepath: str):
        """Markdown í˜•ì‹ìœ¼ë¡œ export"""
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(f"# {data.get('title', 'ì œëª© ì—†ìŒ')}\n\n")
            f.write(f"**ì‘ì„±ì**: {data.get('author', 'ì•Œ ìˆ˜ ì—†ìŒ')}\n")
            f.write(f"**ì‘ì„±ì¼**: {data.get('date', 'ë‚ ì§œ ì—†ìŒ')}\n\n")
            f.write("---\n\n")
            f.write(data.get('content', ''))
            
            if data.get('images'):
                f.write("\n\n## ì´ë¯¸ì§€\n\n")
                for img in data['images']:
                    f.write(f"![{img.get('alt', '')}]({img.get('local_path', '')})\n")
```

### Phase 6: Rate Limiting ë° ì—ëŸ¬ ì²˜ë¦¬ (Day 7)
#### 6.1 Rate Limiter êµ¬í˜„
```python
# src/utils/rate_limiter.py
import time
from functools import wraps
from typing import Callable
import random

class RateLimiter:
    """ìš”ì²­ ì†ë„ ì œí•œ"""
    
    def __init__(self, min_delay: float = 1.0, max_delay: float = 3.0):
        self.min_delay = min_delay
        self.max_delay = max_delay
        self.last_request_time = 0
        
    def wait(self):
        """ìš”ì²­ ê°„ ëŒ€ê¸°"""
        current_time = time.time()
        time_since_last = current_time - self.last_request_time
        
        # ëœë¤ ì§€ì—° ì‹œê°„
        delay = random.uniform(self.min_delay, self.max_delay)
        
        if time_since_last < delay:
            time.sleep(delay - time_since_last)
            
        self.last_request_time = time.time()
        
def rate_limit(min_delay: float = 1.0, max_delay: float = 3.0):
    """ë°ì½”ë ˆì´í„° ë°©ì‹ rate limiting"""
    def decorator(func: Callable):
        limiter = RateLimiter(min_delay, max_delay)
        
        @wraps(func)
        def wrapper(*args, **kwargs):
            limiter.wait()
            return func(*args, **kwargs)
        return wrapper
    return decorator
```

#### 6.2 ì—ëŸ¬ ì²˜ë¦¬ ë° ì¬ì‹œë„ ë¡œì§
```python
# src/utils/error_handler.py
import time
from functools import wraps
from typing import Callable, Tuple, Type
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class RetryHandler:
    """ì¬ì‹œë„ ë¡œì§ ì²˜ë¦¬"""
    
    @staticmethod
    def retry(
        max_attempts: int = 3,
        delay: float = 1.0,
        backoff: float = 2.0,
        exceptions: Tuple[Type[Exception], ...] = (Exception,)
    ):
        """ì¬ì‹œë„ ë°ì½”ë ˆì´í„°"""
        def decorator(func: Callable):
            @wraps(func)
            def wrapper(*args, **kwargs):
                attempt = 1
                current_delay = delay
                
                while attempt <= max_attempts:
                    try:
                        return func(*args, **kwargs)
                    except exceptions as e:
                        if attempt == max_attempts:
                            logger.error(f"Max attempts reached for {func.__name__}: {e}")
                            raise
                            
                        logger.warning(f"Attempt {attempt} failed: {e}. Retrying in {current_delay}s...")
                        time.sleep(current_delay)
                        current_delay *= backoff
                        attempt += 1
                        
            return wrapper
        return decorator
```

### Phase 7: ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ í†µí•© (Day 8)
#### 7.1 í†µí•© ìŠ¤í¬ë˜í¼ í´ë˜ìŠ¤
```python
# src/main_scraper.py
from scraper.url_parser import NaverBlogURLParser
from scraper.content_extractor import NaverBlogContentExtractor
from scraper.image_downloader import ImageDownloader
from storage.data_storage import DataStorage
from utils.rate_limiter import rate_limit
from utils.error_handler import RetryHandler
import logging

class NaverBlogScraper:
    """ë„¤ì´ë²„ ë¸”ë¡œê·¸ ìŠ¤í¬ë˜í¼ ë©”ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self, headless: bool = True):
        self.url_parser = NaverBlogURLParser()
        self.content_extractor = NaverBlogContentExtractor(headless)
        self.image_downloader = ImageDownloader()
        self.storage = DataStorage()
        self.logger = logging.getLogger(__name__)
        
    @RetryHandler.retry(max_attempts=3, delay=2.0)
    @rate_limit(min_delay=2.0, max_delay=5.0)
    def scrape_blog(self, url: str) -> Dict:
        """ë¸”ë¡œê·¸ ìŠ¤í¬ë˜í•‘ ì‹¤í–‰"""
        try:
            # URL íŒŒì‹±
            url_info = self.url_parser.parse_url(url)
            self.logger.info(f"Scraping blog: {url_info['blog_id']}/{url_info['log_no']}")
            
            # ì»¨í…ì¸  ì¶”ì¶œ
            content = self.content_extractor.extract_content(url_info['normalized_url'])
            
            # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
            if content.get('images'):
                downloaded_images = self.image_downloader.download_images(
                    content['images'],
                    url_info['blog_id'],
                    url_info['log_no']
                )
                content['images'] = downloaded_images
                
            # ë©”íƒ€ë°ì´í„° ì¶”ê°€
            content.update(url_info)
            
            # ë°ì´í„° ì €ì¥
            self.storage.save_to_database(content)
            self.storage.save_to_json(
                content, 
                f"data/exports/{url_info['blog_id']}_{url_info['log_no']}.json"
            )
            
            self.logger.info(f"Successfully scraped: {content.get('title')}")
            return content
            
        except Exception as e:
            self.logger.error(f"Failed to scrape {url}: {e}")
            raise
            
    def scrape_multiple(self, urls: List[str]) -> List[Dict]:
        """ì—¬ëŸ¬ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìŠ¤í¬ë˜í•‘"""
        results = []
        for url in urls:
            try:
                result = self.scrape_blog(url)
                results.append(result)
            except Exception as e:
                self.logger.error(f"Failed to scrape {url}: {e}")
                continue
                
        return results
        
    def __del__(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if hasattr(self, 'content_extractor'):
            self.content_extractor.driver.quit()
```

### Phase 8: í…ŒìŠ¤íŠ¸ ì½”ë“œ ì‘ì„± (Day 9)
#### 8.1 ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
```python
# tests/test_scraper.py
import pytest
from src.scraper.url_parser import NaverBlogURLParser
from src.scraper.content_extractor import NaverBlogContentExtractor

class TestURLParser:
    def test_parse_standard_url(self):
        parser = NaverBlogURLParser()
        result = parser.parse_url("https://blog.naver.com/test_blog/123456789")
        assert result['blog_id'] == 'test_blog'
        assert result['log_no'] == '123456789'
        
    def test_parse_mobile_url(self):
        parser = NaverBlogURLParser()
        result = parser.parse_url("https://m.blog.naver.com/test_blog/123456789")
        assert result['blog_id'] == 'test_blog'
        assert result['log_no'] == '123456789'
        
    def test_invalid_url(self):
        parser = NaverBlogURLParser()
        with pytest.raises(ValueError):
            parser.parse_url("https://invalid-site.com/blog")

class TestContentExtractor:
    @pytest.mark.integration
    def test_extract_content(self):
        extractor = NaverBlogContentExtractor(headless=True)
        # ì‹¤ì œ ë¸”ë¡œê·¸ URLë¡œ í…ŒìŠ¤íŠ¸ (í…ŒìŠ¤íŠ¸ìš© ë¸”ë¡œê·¸ í•„ìš”)
        content = extractor.extract_content("https://blog.naver.com/test_blog/test_post")
        
        assert 'title' in content
        assert 'content' in content
        assert 'images' in content
```

### Phase 9: CLI ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„ (Day 10)
```python
# cli.py
import argparse
import sys
from src.main_scraper import NaverBlogScraper

def main():
    parser = argparse.ArgumentParser(description='ë„¤ì´ë²„ ë¸”ë¡œê·¸ ìŠ¤í¬ë˜í¼')
    parser.add_argument('url', help='ë„¤ì´ë²„ ë¸”ë¡œê·¸ URL')
    parser.add_argument('--output', '-o', default='json', 
                       choices=['json', 'csv', 'markdown'],
                       help='ì¶œë ¥ í˜•ì‹')
    parser.add_argument('--no-images', action='store_true',
                       help='ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ê±´ë„ˆë›°ê¸°')
    parser.add_argument('--headless', action='store_true',
                       help='í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œë¡œ ì‹¤í–‰')
    
    args = parser.parse_args()
    
    scraper = NaverBlogScraper(headless=args.headless)
    
    try:
        result = scraper.scrape_blog(args.url)
        print(f"âœ… ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: {result.get('title')}")
        print(f"ğŸ“ ë³¸ë¬¸ ê¸¸ì´: {len(result.get('content', ''))} ì")
        print(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ ìˆ˜: {len(result.get('images', []))}")
        
    except Exception as e:
        print(f"âŒ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
```

## ğŸš€ ì‹¤í–‰ ë°©ë²•

### ì„¤ì¹˜
```bash
# ê°€ìƒí™˜ê²½ ìƒì„±
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt

# Playwright ë¸Œë¼ìš°ì € ì„¤ì¹˜ (Playwright ì‚¬ìš©ì‹œ)
playwright install chromium
```

### ê¸°ë³¸ ì‚¬ìš©ë²•
```bash
# ë‹¨ì¼ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìŠ¤í¬ë˜í•‘
python cli.py "https://blog.naver.com/example/123456789"

# JSONìœ¼ë¡œ ì €ì¥
python cli.py "https://blog.naver.com/example/123456789" --output json

# ì´ë¯¸ì§€ ì œì™¸í•˜ê³  ìŠ¤í¬ë˜í•‘
python cli.py "https://blog.naver.com/example/123456789" --no-images

# í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ
python cli.py "https://blog.naver.com/example/123456789" --headless
```

### Python ì½”ë“œì—ì„œ ì‚¬ìš©
```python
from src.main_scraper import NaverBlogScraper

# ìŠ¤í¬ë˜í¼ ì´ˆê¸°í™”
scraper = NaverBlogScraper(headless=True)

# ë‹¨ì¼ ë¸”ë¡œê·¸ ìŠ¤í¬ë˜í•‘
result = scraper.scrape_blog("https://blog.naver.com/example/123456789")

# ì—¬ëŸ¬ ë¸”ë¡œê·¸ ìŠ¤í¬ë˜í•‘
urls = [
    "https://blog.naver.com/example1/111111111",
    "https://blog.naver.com/example2/222222222"
]
results = scraper.scrape_multiple(urls)
```

## âš ï¸ ì£¼ì˜ì‚¬í•­

### ë²•ì /ìœ¤ë¦¬ì  ê³ ë ¤ì‚¬í•­
1. **ì €ì‘ê¶Œ**: ìŠ¤í¬ë˜í•‘í•œ ì»¨í…ì¸ ì˜ ì €ì‘ê¶Œì€ ì›ì‘ìì—ê²Œ ìˆìŒ
2. **ê°œì¸ì •ë³´**: ê°œì¸ì •ë³´ê°€ í¬í•¨ëœ ë°ì´í„° ìˆ˜ì§‘ ì£¼ì˜
3. **ì„œë¹„ìŠ¤ ì•½ê´€**: ë„¤ì´ë²„ ì„œë¹„ìŠ¤ ì´ìš©ì•½ê´€ ì¤€ìˆ˜
4. **robots.txt**: ì›¹ì‚¬ì´íŠ¸ì˜ í¬ë¡¤ë§ ì •ì±… í™•ì¸

### ê¸°ìˆ ì  ê³ ë ¤ì‚¬í•­
1. **Rate Limiting**: ê³¼ë„í•œ ìš”ì²­ìœ¼ë¡œ IP ì°¨ë‹¨ ë°©ì§€
2. **User-Agent**: ì ì ˆí•œ User-Agent ì„¤ì •
3. **ë™ì  ì»¨í…ì¸ **: JavaScript ë Œë”ë§ ëŒ€ê¸° ì‹œê°„ í•„ìš”
4. **ë„¤íŠ¸ì›Œí¬ ì—ëŸ¬**: ì¬ì‹œë„ ë¡œì§ ë° ì˜ˆì™¸ ì²˜ë¦¬

## ğŸ“Š ì„±ëŠ¥ ìµœì í™”

### ë¹„ë™ê¸° ì²˜ë¦¬ (ê³ ê¸‰)
```python
import asyncio
from concurrent.futures import ThreadPoolExecutor

async def scrape_async(urls: List[str]):
    with ThreadPoolExecutor(max_workers=5) as executor:
        loop = asyncio.get_event_loop()
        tasks = [
            loop.run_in_executor(executor, scraper.scrape_blog, url)
            for url in urls
        ]
        return await asyncio.gather(*tasks)
```

### ìºì‹± ì „ëµ
- ì´ë¯¸ ìŠ¤í¬ë˜í•‘í•œ URL ì¶”ì 
- ì´ë¯¸ì§€ ì¤‘ë³µ ë‹¤ìš´ë¡œë“œ ë°©ì§€
- ë°ì´í„°ë² ì´ìŠ¤ ì¸ë±ì‹±

## ğŸ”§ í™•ì¥ ê°€ëŠ¥ì„±

### ì¶”ê°€ ê¸°ëŠ¥ êµ¬í˜„ ì•„ì´ë””ì–´
1. **ëŒ“ê¸€ ì¶”ì¶œ**: ë¸”ë¡œê·¸ ëŒ“ê¸€ ìŠ¤í¬ë˜í•‘
2. **í†µê³„ ë¶„ì„**: ë¸”ë¡œê·¸ í†µê³„ ì •ë³´ ìˆ˜ì§‘
3. **ì¹´í…Œê³ ë¦¬ë³„ ìˆ˜ì§‘**: íŠ¹ì • ì¹´í…Œê³ ë¦¬ ì „ì²´ í¬ìŠ¤íŠ¸ ìˆ˜ì§‘
4. **ìŠ¤ì¼€ì¤„ë§**: ì •ê¸°ì ì¸ ì—…ë°ì´íŠ¸ í™•ì¸
5. **API ì„œë²„**: RESTful APIë¡œ ì œê³µ
6. **ì›¹ ì¸í„°í˜ì´ìŠ¤**: Flask/FastAPI ê¸°ë°˜ ì›¹ UI

## ğŸ“ ë¬¸ì„œí™” ë° ìœ ì§€ë³´ìˆ˜

### ë¡œê¹… ì„¤ì •
```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('scraper.log'),
        logging.StreamHandler()
    ]
)
```

### ëª¨ë‹ˆí„°ë§
- ìŠ¤í¬ë˜í•‘ ì„±ê³µ/ì‹¤íŒ¨ìœ¨
- í‰ê·  ì²˜ë¦¬ ì‹œê°„
- ì—ëŸ¬ ë°œìƒ íŒ¨í„´
- ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰

## ğŸ¯ í’ˆì§ˆ ë³´ì¦

### í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€
- ë‹¨ìœ„ í…ŒìŠ¤íŠ¸: 80% ì´ìƒ
- í†µí•© í…ŒìŠ¤íŠ¸: ì£¼ìš” í”Œë¡œìš°
- E2E í…ŒìŠ¤íŠ¸: ì‹¤ì œ ë¸”ë¡œê·¸ URL

### CI/CD íŒŒì´í”„ë¼ì¸
```yaml
# .github/workflows/test.yml
name: Test
on: [push, pull_request]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - uses: actions/setup-python@v2
      - run: pip install -r requirements.txt
      - run: pytest tests/
```

## ğŸ“š ì°¸ê³  ìë£Œ
- [Selenium Documentation](https://www.selenium.dev/documentation/)
- [BeautifulSoup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- [Playwright Python](https://playwright.dev/python/)
- [ë„¤ì´ë²„ robots.txt](https://blog.naver.com/robots.txt)